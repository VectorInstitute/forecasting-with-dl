{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "prospective-broad",
   "metadata": {},
   "source": [
    "![Forecasting Demo 1: Baselines, Prophet, and NeuralProphet](https://raw.githubusercontent.com/VectorInstitute/forecasting-bootcamp/media-assets-do-not-merge/forecasting-demo-1.png?token=GHSAT0AAAAAABQMCWQFQHUMDN4MVB2LEQDUYQ7WXUQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-utilization",
   "metadata": {
    "id": "SYomKW1s04Sg"
   },
   "source": [
    "This notebook is the first of a series that introduces the application of popular, recently developed time series forecasting methods. In particular, we emphasize the use of consistent evaluation metrics and analysis across all models and model configurations. \n",
    "\n",
    "Use these notebooks as tools to explore the application of various forecasting methods to multivariate time series datasets, and to inspire an experimental approach for comparing multiple models and model configurations.\n",
    "\n",
    "This notebook explores the application of **Prophet** and **NeuralProphet** to exchange rate forecasting, as well as two baseline methods using **sktime**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-pollution",
   "metadata": {
    "id": "inside-wireless"
   },
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    !pip install prophet\n",
    "    !pip install git+https://github.com/ourownstory/neural_prophet.git # may take a while\n",
    "    #!pip install neuralprophet # much faster, but may not have the latest upgrades/bugfixes\n",
    "    !pip install sktime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sktime.forecasting.naive import NaiveForecaster\n",
    "from prophet import Prophet\n",
    "from neuralprophet import NeuralProphet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turned-sunset",
   "metadata": {
    "id": "MsovnLdhfwsu"
   },
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varied-cursor",
   "metadata": {
    "id": "diverse-eight"
   },
   "source": [
    "### Load exchange rate data file\n",
    "\n",
    "The used dataset includes daily exchange rates between CAD and 12 other currencies between 2007 and 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-gauge",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XUvJWReJwcMb",
    "outputId": "f36cc9d8-747b-489c-9a67-2bc82dc6280b"
   },
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuck-ethernet",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "german-pilot",
    "outputId": "84561824-ea24-49ff-a829-46b1a21a563e"
   },
   "outputs": [],
   "source": [
    "# your Google Drive path may also begin with /content/drive/MyDrive/\n",
    "data_filename = \"/ssd003/projects/forecasting_bootcamp/bootcamp_datasets/boc_exchange/dataset.csv\"\n",
    "data_df = pd.read_csv(data_filename, index_col=0)\n",
    "data_df.index = pd.to_datetime(data_df.index)\n",
    "data_df = data_df.reset_index().rename({'index':'date'}, axis=1)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impossible-latin",
   "metadata": {
    "id": "lined-nicholas"
   },
   "source": [
    "### Split data according to use case\n",
    "\n",
    "For simplicity, this notebook uses a conventional training and testing split over the dataset. Other notebooks will give examples of rolling cross validation using multiple validation periods given by a set of cutoff dates. \n",
    "\n",
    "The purpose of this notebook is to explore a simpler problem formulation using multiple models. The experiments and analysis can be easily adapted for rolling cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-treasurer",
   "metadata": {
    "id": "raised-uniform"
   },
   "outputs": [],
   "source": [
    "lag_time = 90\n",
    "lead_time = 60\n",
    "\n",
    "train_size = 0.8\n",
    "\n",
    "train_df = data_df.iloc[:int(len(data_df)*train_size)]\n",
    "test_df = data_df.iloc[int(len(data_df)*train_size):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-stick",
   "metadata": {},
   "source": [
    "To ensure that we have enough data for testing, we need to withhold at least `lag_time + lead_time` observations from the dataset. Assuming we want to test a fitted model on all available examples in the test set, the number of testing examples can be computed as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unnecessary-measure",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test_cases = len(test_df) - lag_time - lead_time + 1\n",
    "print(f\"   Timesteps in test_df: {len(test_df)}\")\n",
    "print(f\"Number of test examples: {n_test_cases}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-aluminum",
   "metadata": {},
   "source": [
    "### Iterating over test examples\n",
    "\n",
    "To help with iterating over valid pairs of input and target data, we define a PyTorch-like dataset class. In this notebook, we'll use this primarily for iterating over test examples, since both Prophet and NeuralProphet impose their own, special formats for passing in training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-fever",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForecastingDataset:\n",
    "\n",
    "    def __init__(self, data_df, lag_time, lead_time, feature_columns):\n",
    "        self.n_examples = len(data_df) - lag_time - lead_time + 1\n",
    "        assert self.n_examples > 0, \"Dataset must contain at least one example.\"\n",
    "        assert \"date\" in data_df.columns or \"ds\" in data_df.columns, \"Source DataFrame must contain a date/ds column.\"\n",
    "\n",
    "        self.df = data_df[feature_columns]\n",
    "        if 'date' in data_df.columns:\n",
    "            self.dates = data_df.date\n",
    "        elif 'ds' in data_df.columns:\n",
    "            self.dates = data_df.ds\n",
    "        self.lag_time = lag_time\n",
    "        self.lead_time = lead_time\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_examples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input = self.df.iloc[idx:idx+lag_time]\n",
    "        output = self.df.iloc[idx+lag_time:idx+lag_time+lead_time]\n",
    "        input_dates = self.dates[idx:idx+lag_time]\n",
    "        output_dates = self.dates[idx+lag_time:idx+lag_time+lead_time]\n",
    "        return input, output, input_dates, output_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-orientation",
   "metadata": {},
   "source": [
    "Next, we instantiate an indexable `test_dataset`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-floor",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [col for col in test_df if col.endswith(\"_CLOSE\")]\n",
    "test_dataset = ForecastingDataset(test_df, lag_time, lead_time, feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-northwest",
   "metadata": {
    "id": "announced-debate"
   },
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "In order to objectively compare the performance of this and other models on out-of-sample forecasting performance, we will need to collect output in a consistent format and apply a suite of standard evaluation metrics:\n",
    "\n",
    "- Mean Squared Error (MSE)\n",
    "- Root Mean Squared Error (RMSE)\n",
    "- Mean Absolute Error (MAE)\n",
    "- Mean Absolute Percentage Error (MAPE)\n",
    "\n",
    "See the article [Time Series Forecast Error Metrics You Should Know](https://towardsdatascience.com/time-series-forecast-error-metrics-you-should-know-cc88b8c67f27) for an overview of these and other popular forecasting error metrics. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-nothing",
   "metadata": {
    "id": "ambient-drink"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "metrics = {\n",
    "    'mse': mean_squared_error,\n",
    "    'rmse': lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "    'mae': mean_absolute_error,\n",
    "    'mape': mean_absolute_percentage_error\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-supplier",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error_statistics(error_metrics_dict, exp_name):\n",
    "    return {\n",
    "        'mean': pd.DataFrame(error_metrics_dict).mean(axis=0).rename(f'{exp_name}_mean_metrics'),\n",
    "        'std': pd.DataFrame(error_metrics_dict).std(axis=0).rename(f'{exp_name}_std_metrics'),\n",
    "        'max': pd.DataFrame(error_metrics_dict).max(axis=0).rename(f'{exp_name}_max_metrics'),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-prague",
   "metadata": {},
   "source": [
    "# Baseline Forecasts\n",
    "\n",
    "Let's begin our experiments by producing forecasts using na√Øve estimators. A common baseline is *persistence forecasting*, where the forecast is simply an extension of the last known observation of the time series. A second baseline is the *mean window forecast*, where we take the mean over a window of observations and use this value for forecasts. The following code produces and collects the baseline forecasts into lists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-smooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model_persistence = NaiveForecaster(strategy='last')\n",
    "baseline_model_mean = NaiveForecaster(strategy='mean',window_length=lag_time)\n",
    "\n",
    "forecasts_persistence = []\n",
    "forecasts_mean = []\n",
    "\n",
    "for i in range(len(test_dataset)):\n",
    "    x, y, x_d, y_d = test_dataset[i]\n",
    "    \n",
    "    persistence_fc = baseline_model_persistence.fit_predict(x['USD_CLOSE'], fh=list(range(1, lead_time+1)))\n",
    "    persistence_fc = pd.Series(persistence_fc.values, index=y_d)\n",
    "    forecasts_persistence.append(persistence_fc)\n",
    "\n",
    "    mean_fc = baseline_model_mean.fit_predict(x['USD_CLOSE'], fh=list(range(lead_time)))\n",
    "    mean_fc = pd.Series(mean_fc.values, index=y_d)\n",
    "    forecasts_mean.append(mean_fc)\n",
    "\n",
    "    print(i, end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-garage",
   "metadata": {},
   "source": [
    "### Compute error metrics over the baseline forecasts\n",
    "\n",
    "In this notebook, we want to compare the performance of experimental models (Prophet, NeuralProphet) compared to baselines (persistence and mean window extension). The following code applies each of the four evaluation metrics for every example in the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-moderator",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_baseline_error_metrics(forecasts, test_dataset):\n",
    "\n",
    "    errors = {metric_name:[] for metric_name in metrics.keys()}\n",
    "\n",
    "    for i in range(len(forecasts)):\n",
    "        \n",
    "        fc = forecasts[i]\n",
    "        x, y, x_d, y_d = test_dataset[i]\n",
    "    \n",
    "        for metric_name, metric_fn in metrics.items(): \n",
    "                errors[metric_name].append(metric_fn(y_true=y['USD_CLOSE'], y_pred=fc))\n",
    "\n",
    "    return errors, forecasts\n",
    "\n",
    "persistence_errors, _ = compute_baseline_error_metrics(forecasts_persistence, test_dataset)\n",
    "mean_errors, _ = compute_baseline_error_metrics(forecasts_mean, test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-chase",
   "metadata": {},
   "source": [
    "The following code uses the function `compute_error_statistics` to reduce the mean evaluation metrics over the entire test set to three statistics (mean, standard deviation, and max)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuous-tours",
   "metadata": {},
   "outputs": [],
   "source": [
    "persistence_stats = compute_error_statistics(persistence_errors, 'persistence')\n",
    "persistence_stats['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "allied-deficit",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_window_stats = compute_error_statistics(mean_errors, 'mean_window')\n",
    "mean_window_stats['mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-baltimore",
   "metadata": {},
   "source": [
    "We now collect the mean evaluation statistics for each metric into a DataFrame so that we can later compare these to experimental models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-carrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(persistence_stats['mean']).T\n",
    "results_df = results_df.append(mean_window_stats['mean'])\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-approach",
   "metadata": {},
   "source": [
    "### Visualizing forecasts over the test set\n",
    "\n",
    "For each example in the test set, we have produced a forecast between `1` and `lead_time` days into the future. As we will see later, this is difficult to visualize over the whole test set. Instead, we can visualize the value of each forecast at a single time step into the future. The code below visualizes the baseline forecasts at the maximum lead time. As we can see, the persistence forecast is exactly the ground truth shifted `lead_time` days into the future. In the context of exchange rate forecasting, this baseline may be difficult to beat.\n",
    "\n",
    "#### Persistence Forecasts At Max Lead Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-creek",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_fcs = [{'date': fc.index[-1:][0], 'yhat':fc[-1:][0]} for fc in forecasts_persistence]\n",
    "max_fcs = pd.DataFrame(max_fcs)\n",
    "\n",
    "plt.figure(figsize=(12,3))\n",
    "plt.plot(test_df.date, test_df['USD_CLOSE'], color='blue', label='ground truth')\n",
    "plt.plot(max_fcs.date, max_fcs.yhat, color='red', label='forecast')\n",
    "plt.title(f\"Forecasts at max lead time ({lead_time} samples) - Persistence\")\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-pioneer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ground truth\n",
    "plt.figure(figsize=(12,3))\n",
    "ground_truth = test_df[['date', 'USD_CLOSE']]\n",
    "plt.plot(ground_truth.date, ground_truth['USD_CLOSE'], label='ground truth')\n",
    "\n",
    "# Plot example single forecast\n",
    "plt.plot(forecasts_persistence[-1], label='forecast')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-queensland",
   "metadata": {},
   "source": [
    "#### Mean Window Forecasts At Max Lead Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awful-diagram",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_fcs = [{'date': fc.index[-1:][0], 'yhat':fc[-1:][0]} for fc in forecasts_mean]\n",
    "max_fcs = pd.DataFrame(max_fcs)\n",
    "\n",
    "plt.figure(figsize=(12,3))\n",
    "plt.plot(test_df.date, test_df['USD_CLOSE'], color='blue', label='ground truth')\n",
    "plt.plot(max_fcs.date, max_fcs.yhat, color='red', label='forecast')\n",
    "plt.title(f\"Forecasts at max lead time ({lead_time} samples) - Mean Window\")\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-quick",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ground truth\n",
    "plt.figure(figsize=(12,3))\n",
    "ground_truth = test_df[['date', 'USD_CLOSE']]\n",
    "plt.plot(ground_truth.date, ground_truth['USD_CLOSE'], label='ground truth')\n",
    "\n",
    "# Plot example single forecast\n",
    "plt.plot(forecasts_mean[-1], label='forecast')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "popular-newsletter",
   "metadata": {
    "id": "latin-atlas"
   },
   "source": [
    "# Prophet\n",
    "\n",
    "Univariate forecasting that supports additional *future* regressors. Prophet does not support the inclusion of *lagged regressors*, i.e. it does not support the use of historical values of multiple series to predict a single target series. We include it as a baseline because it is popular, lightweight, interpretable, and performs very well in some domains. \n",
    "\n",
    "Prophet is based on a Generalized Additive Model (GAM):\n",
    "\n",
    "$ y(t) = g(t) + s(t) + h(t) + \\epsilon_t$\n",
    "\n",
    "where $y(t)$ is the target series, $g(t)$ is the trend function, $s(t)$ is the seasonality or periodic function, $h(t)$ is a function reflecting holidays or other irregular events, and $\\epsilon_t$ is an error term that is assumed to be normally distributed.\n",
    "\n",
    "Despite being formulated as an additive model, multiplicative interaction between seasonality and trend components is supported (using a log transform). In the implementation, this is easily configurable using a constructor parameter. See the [documentation](https://facebook.github.io/prophet/docs/multiplicative_seasonality.html) for more details.\n",
    "\n",
    "#### Data Preparation\n",
    "\n",
    "Prophet, like most forecasting packages, imposes its own, specific format for input data. It expects inputs in the form of a Pandas DataFrame with two columns, `ds` and `y`, which correspond to Pandas-formatted timestamps and the target time series, respectively.\n",
    "\n",
    "In this example, we create a Prophet DataFrame by selecting the columns `date` and `USD_CLOSE` from the Bank of Canada exchange rate dataset. We then rename those columns to `ds` and `y`, respectively. \n",
    "\n",
    "Note that the `ds` column is already correctly formatted using the Pandas datetime format, since we converted it immediately after loading the data. When reading CSVs, always be sure to check that datestamps are properly formatted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-numbers",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "generous-glasgow",
    "outputId": "90affa3b-a381-42a5-e6db-72f5160a3640"
   },
   "outputs": [],
   "source": [
    "prophet_model_df = train_df[['date', 'USD_CLOSE']]\n",
    "prophet_model_df = train_df.rename({'date':'ds', 'USD_CLOSE':'y'}, axis=1)\n",
    "prophet_model_df = prophet_model_df[['ds', 'y']]\n",
    "prophet_model_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-allocation",
   "metadata": {
    "id": "ideal-injury"
   },
   "source": [
    "### Model Initialization and Fitting\n",
    "\n",
    "For our baseline model, we fit Prophet using its default configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-arrest",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "blond-stamp",
    "outputId": "ac3aaa63-501a-45b4-b715-77e569bd5c6f"
   },
   "outputs": [],
   "source": [
    "model = Prophet()\n",
    "model = model.fit(prophet_model_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-austria",
   "metadata": {
    "id": "facial-tracker"
   },
   "source": [
    "### Produce Forecasts\n",
    "\n",
    "To produce a forecast using a fitted Prophet model, we need to pass it a dataframe with the desired timestamps in a column named `ds`. In the example below, we use the fitted model object to produce a dataframe `future` with dates that extend `len(test_df)` days beyond the training dates. Passing `future` to the fitted model's `predict` function will return a dataframe populated with a detailed forecast, including model component values and confidence ranges.\n",
    "\n",
    "Notice here that we are asking Prophet to produce a single forecast for the entire test period. We are doing this because Prophet does not support inference using fixed-sized inputs in the same way that every other technique considered in our bootcamp does. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finished-visit",
   "metadata": {
    "id": "twenty-ghana"
   },
   "outputs": [],
   "source": [
    "future = model.make_future_dataframe(periods=len(test_df))\n",
    "forecast = model.predict(future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-broadcast",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "id": "identical-affair",
    "outputId": "49d90418-c170-47d3-c353-5ad5c5fdd4f2"
   },
   "outputs": [],
   "source": [
    "forecast.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entitled-elements",
   "metadata": {
    "id": "racial-section"
   },
   "source": [
    "### Plotting Prophet Forecasts\n",
    "\n",
    "The following code visualizes the application of the fitted Prophet model to both in-sample (training) and out-of-sample (testing) data. Visualization and evaluation of forecasting models using out-of-sample data is crucial for estimating future performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verified-brighton",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "undefined-drove",
    "outputId": "3532204b-1528-4959-dd55-cee4cdf913b6"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "ax.fill_between(forecast.ds.iloc[:-len(test_df)], \n",
    "    forecast.yhat_lower.iloc[:-len(test_df)], \n",
    "    forecast.yhat_upper.iloc[:-len(test_df)],\n",
    "    color='blue', label='In-Sample confidence interval (80%)', alpha=0.15)\n",
    "\n",
    "ax.fill_between(forecast.ds.iloc[-len(test_df):], \n",
    "    forecast.yhat_lower.iloc[-len(test_df):], \n",
    "    forecast.yhat_upper.iloc[-len(test_df):],\n",
    "    color='red', label='Out-of-Sample confidence interval (80%)', alpha=0.1)\n",
    "\n",
    "ax.scatter(prophet_model_df.ds, prophet_model_df['y'], color='slategrey', s=3, linewidths=0, label='Train Samples')\n",
    "ax.scatter(test_df.date, test_df['USD_CLOSE'], color='salmon', s=3, linewidths=0, label='Test Samples')\n",
    "\n",
    "ax.plot(forecast.ds.iloc[:-len(test_df)], \n",
    "        forecast.yhat.iloc[:-len(test_df)], color='blue', label='In-Sample Forecast')\n",
    "\n",
    "ax.plot(forecast.ds.iloc[-len(test_df):], forecast.yhat.iloc[-len(test_df):], \n",
    "        color='red', label='Out-of-Sample Forecast')\n",
    "\n",
    "ax.legend(loc='upper left')\n",
    "ax.grid(axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternate-royalty",
   "metadata": {},
   "source": [
    "### Prophet Forecasts At Max Lead Time\n",
    "\n",
    "As we did with the baseline methods, let's visualize Prophet's forecasts at maximum lead time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "three-payroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use our ForecastingDataset class to help with formatting Prophet's output.\n",
    "forecast_eval_dataset = ForecastingDataset(forecast.iloc[-len(test_df):], lag_time, lead_time, ['yhat'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,4))\n",
    "\n",
    "forecasts_at_max_lead = []\n",
    "dates_at_max_lead = []\n",
    "\n",
    "for i in range(len(forecast_eval_dataset)):\n",
    "    x, y, x_d, y_d = forecast_eval_dataset[i]\n",
    "    x_gt, y_gt, x_gt_d, y_gt_d = test_dataset[i]\n",
    "\n",
    "    forecasts_at_max_lead.append(y.values[-1])\n",
    "    dates_at_max_lead.append(y_d.values[-1])\n",
    "\n",
    "ax.plot(dates_at_max_lead, forecasts_at_max_lead, color='red', label='forecast')\n",
    "ax.plot(test_df.date, test_df['USD_CLOSE'], color='blue', label='ground truth')\n",
    "plt.legend()\n",
    "plt.title(f\"Forecasts at max lead time ({lead_time} samples) - Prophet\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banner-battery",
   "metadata": {},
   "source": [
    "With the help of the ForecastingDataset class defined earlier, iterate over each forecast and ground truth pair, and compute and collect multiple evaluation metrics as defined in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-stack",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error_metrics(ground_truth_dataset, forecast_dataset):\n",
    "        \n",
    "    errors = {metric_name:[] for metric_name in metrics.keys()}\n",
    "\n",
    "    for i in range(len(forecast_dataset)):\n",
    "        x, y, x_d, y_d = forecast_dataset[i]\n",
    "        x_gt, y_gt, x_gt_d, y_gt_d = ground_truth_dataset[i]\n",
    "        for metric_name, metric_fn in metrics.items(): \n",
    "            errors[metric_name].append(metric_fn(y_true=y_gt['USD_CLOSE'], y_pred=y))\n",
    "    \n",
    "    return errors\n",
    "\n",
    "error_metrics = compute_error_metrics(test_dataset, forecast_eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frequent-horse",
   "metadata": {},
   "outputs": [],
   "source": [
    "prophet_stats = compute_error_statistics(error_metrics, 'prophet')\n",
    "prophet_stats['mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reserved-cyprus",
   "metadata": {
    "id": "speaking-physiology"
   },
   "source": [
    "Let's now collect the mean evaluation metrics into a new DataFrame that we will use for comparative evalution against other models' forecasts.\n",
    "\n",
    "Please note that the comparison is not completely fair - Prophet has to predict 672 steps into the future at once, whereas our baselines only have to predict the next 60 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-denial",
   "metadata": {
    "id": "canadian-coating"
   },
   "outputs": [],
   "source": [
    "results_df = results_df.append(prophet_stats['mean'])\n",
    "results_df.sort_values('mae')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attended-airline",
   "metadata": {
    "id": "interracial-karen"
   },
   "source": [
    "# NeuralProphet\n",
    "\n",
    "Let's proceed to explore the [NeuralProphet](https://neuralprophet.com/html/index.html) model. Please review the following resources to learn more:\n",
    "\n",
    "- [Paper](https://arxiv.org/abs/2111.15397)\n",
    "- [Documentation](https://neuralprophet.com/html/contents.html)\n",
    "- [GitHub](https://github.com/ourownstory/neural_prophet)\n",
    "\n",
    "In the words of its developers, NeuralProphet is \"*based on neural networks, inspired by Facebook Prophet and AR-Net, built on PyTorch*\". A very important differentiating feature is that NeuralProphet conveniently supports *lagged regressors*. In the context of this running example, NeuralProphet supports the use of multiple other currencies' time series. With this expanded flexibility, however, the model is more complex, with a greater number of design choices and hyperparameters to consider. \n",
    "\n",
    "The official [documentation on lagged regressors (lagged covariates)](https://neuralprophet.com/html/lagged_covariates_energy_ercot.html) gives several examples for configuring NeuralProphet models to use lagged regressors, but commentary and suggestions on best practices are largely absent. \n",
    "\n",
    "In the following code, we will consider a small number of NeuralProphet model configurations applied to the same forecasting task from above. Importantly, we retain the same train/test (in-sample/out-of-sample) split, and we will apply the same evaluation metrics to NeuralProphet's forecasts. \n",
    "\n",
    "### Data Formatting\n",
    "\n",
    "NeuralProphet's data format is very similar to Prophet's. We prepare new DataFrames for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "killing-cache",
   "metadata": {
    "id": "placed-cooper"
   },
   "outputs": [],
   "source": [
    "np_train_df = train_df.reset_index().rename({'date':'ds', 'USD_CLOSE':'y'}, axis=1).drop('index', axis=1)\n",
    "np_test_df = test_df.reset_index().rename({'date':'ds','USD_CLOSE':'y'}, axis=1).drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-nepal",
   "metadata": {},
   "source": [
    "Of course the most important difference between the DataFrames prepared for Prophet and NeuralProphet is that, with NeuralProphet, we have the opportunity to include data about the non-target variables as lagged regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-alias",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_train_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-protection",
   "metadata": {
    "id": "round-attitude"
   },
   "source": [
    "## Baseline/Default Model\n",
    "\n",
    "A baseline NeuralProphet model with lagged regressors using default initialization parameters, except:\n",
    "\n",
    "- `n_lags=lag_time`, specifying that the autoregressive component of the model should use the past `lag_time` daily observations as inputs\n",
    "- `n_forecasts=lead_time`, specifying that our use case is to predict the target signal `lead_time` days into the future\n",
    "\n",
    "NeuralProphet also allows you to specify a `validation_df` in `fit()`, on which the model will be evaluated every epoch. We are not using this feature here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affiliated-panama",
   "metadata": {
    "id": "external-detection",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np_model = NeuralProphet(n_lags=lag_time, n_forecasts=lead_time)\n",
    "\n",
    "# Add the non-target feature columns as lagged regressors\n",
    "feature_cols = [col for col in np_train_df if col not in ('USD_CLOSE', 'ds', 'y')]\n",
    "for feature in feature_cols:\n",
    "    np_model.add_lagged_regressor(f'{feature}')\n",
    "    \n",
    "np_model.fit(np_train_df, freq='D')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sound-prefix",
   "metadata": {},
   "source": [
    "After fitting, you can plot the learned model parameters, including the additional 11 lagged regressors (with 90 day lead time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-wyoming",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_model.plot_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-crown",
   "metadata": {
    "id": "TOY5P1Hq2Ned"
   },
   "source": [
    "NeuralProphet, rather annoyingly, does not collect forecasts into a single yhat variable, but rather into separate `stepX`s for each of the lead times. For example, the following is a single 60-day forecast:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-appreciation",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, x_d, y_d = test_dataset[0]\n",
    "x = x.reset_index().rename({'date':'ds', 'USD_CLOSE':'y'}, axis=1).drop('index', axis=1)\n",
    "x = x.assign(ds=x_d.reset_index().drop('index', axis=1).values)\n",
    "y = y.reset_index().rename({'date':'ds', 'USD_CLOSE':'y'}, axis=1).drop('index', axis=1)\n",
    "\n",
    "np_future_df = np_model.make_future_dataframe(x, periods=len(y))\n",
    "np_forecast = np_model.predict(np_future_df, decompose=False, raw=True)\n",
    "np_forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offensive-jewel",
   "metadata": {},
   "source": [
    "To get a more useable data structure, the following function takes a NeuralProphet forecast dataframe and turns it into a time series of its predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-routine",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yhat_from_neuralprophet_forecast(np_forecast, y_d):\n",
    "    return pd.Series(np_forecast.T.iloc[1:].set_index(y_d).iloc[:,0], name='np_yhat').rename_axis('ds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subject-array",
   "metadata": {},
   "source": [
    "The forecast from above would now look this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-wichita",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_from_neuralprophet_forecast(np_forecast, y_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-stylus",
   "metadata": {},
   "source": [
    "Since NeuralProphet uses a fixed-size input sequence (lagged observations) to produce forecasts, we iterate over the input sequences in the test set and use them as model inputs to produce forecasts. This mode of inference should be more familiar to machine learning practitioners than Prophet's. Note that NeuralProphet requires us to first format input data using the `make_future_dataframe` function before running inference using the `predict` function. We define the following function, which produces forecasts for each of the input/ground-truth-output sequences in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italic-edgar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_np_forecasts(np_model, test_dataset):\n",
    "\n",
    "    forecasts = []\n",
    "\n",
    "    for i in range(len(test_dataset)):\n",
    "        \n",
    "        x, y, x_d, y_d = test_dataset[i]\n",
    "        x = x.reset_index().rename({'date':'ds', 'USD_CLOSE':'y'}, axis=1).drop('index', axis=1)\n",
    "        x = x.assign(ds=x_d.reset_index().drop('index', axis=1).values)\n",
    "        y = y.reset_index().rename({'date':'ds', 'USD_CLOSE':'y'}, axis=1).drop('index', axis=1)\n",
    "\n",
    "        np_future_df = np_model.make_future_dataframe(x, periods=len(y))\n",
    "        np_forecast = np_model.predict(np_future_df, decompose=False, raw=True)\n",
    "        fc_series = yhat_from_neuralprophet_forecast(np_forecast, y_d)\n",
    "        forecasts.append(fc_series)\n",
    "\n",
    "    return forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decreased-death",
   "metadata": {},
   "source": [
    "Similarly to what we defined for Prophet, we define the following function for computing and collecting evaluation metrics over all of the forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-tomorrow",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "id": "guided-local",
    "outputId": "3adad78b-e26f-4c49-a9b1-f39439ff1984"
   },
   "outputs": [],
   "source": [
    "def compute_np_error_metrics(forecasts):\n",
    "\n",
    "    errors = {metric_name:[] for metric_name in metrics.keys()}\n",
    "\n",
    "    for i in range(len(forecasts)):\n",
    "        \n",
    "        fc = forecasts[i]\n",
    "        gt = test_df.loc[test_df.date.isin(fc.index)].sort_values('date')  # Sorting because I am not 100% sure that the 'isin' function always preserves order.\n",
    "        \n",
    "        for metric_name, metric_fn in metrics.items(): \n",
    "                errors[metric_name].append(metric_fn(y_true=gt['USD_CLOSE'], y_pred=fc))\n",
    "\n",
    "    return errors, forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-registrar",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = collect_np_forecasts(np_model, test_dataset)\n",
    "np_baseline_error_metrics, fcs = compute_np_error_metrics(forecasts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-relation",
   "metadata": {},
   "source": [
    "### Plot all forecasts\n",
    "\n",
    "We have the option to visualize complete forecasts at every time step, but it does not tell us much about the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-india",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,6))\n",
    "\n",
    "for i in range(len(forecasts)):\n",
    "\n",
    "    fc = forecasts[i]\n",
    "    gt = test_df.loc[test_df.date.isin(fc.index)]\n",
    "\n",
    "    ax.plot(fc.index[:], fc[:], alpha=0.1, color='red')\n",
    "    ax.plot(gt.date, gt['USD_CLOSE'], alpha=0.1, color='blue')\n",
    "plt.title(f\"Forecasts at all lead times (1 to {lead_time} samples)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "favorite-windsor",
   "metadata": {},
   "source": [
    "### Plot all forecasts at max lead time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moving-figure",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_fcs = [{'date': fc.index[-1:][0], 'yhat':fc[-1:][0]} for fc in forecasts]\n",
    "max_fcs = pd.DataFrame(max_fcs)\n",
    "\n",
    "plt.figure(figsize=(12,3))\n",
    "plt.plot(test_df.date, test_df['USD_CLOSE'], color='blue', label='ground truth')\n",
    "plt.plot(max_fcs.date, max_fcs.yhat, color='red', label='forecast')\n",
    "plt.title(f\"Forecasts at max lead time ({lead_time} samples) - Neural Prophet Default\")\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colored-aruba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ground truth\n",
    "plt.figure(figsize=(12,3))\n",
    "ground_truth = test_df[['date', 'USD_CLOSE']]\n",
    "plt.plot(ground_truth.date, ground_truth['USD_CLOSE'], label='ground truth')\n",
    "\n",
    "# Plot example single forecast\n",
    "plt.plot(forecasts[-1], label='forecast')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funny-employer",
   "metadata": {},
   "source": [
    "### Append evaluation metrics to `results_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outdoor-august",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results_df.append(compute_error_statistics(np_baseline_error_metrics, 'neural_prophet_baseline')['mean'])\n",
    "results_df.sort_values('mae')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "important-honor",
   "metadata": {
    "id": "E8ko7L71DXSB"
   },
   "source": [
    "## Restricted model\n",
    "\n",
    "The baseline NeuralProphet model does not perform well on out-of-sample data. We can consider multiple changes to the model's configuration and hyperparameters in pursuit of better performance. Let's consider the following configuration that restricts the model to using only the last observed value of last regressors, as opposed to `n_lags` past observations. While less expressive, this model may be less prone to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-hotel",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_model_last_sample_only = NeuralProphet(n_lags=lag_time, n_forecasts=lead_time)\n",
    "\n",
    "# Add the non-target feature columns as lagged regressors\n",
    "feature_cols = [col for col in np_train_df if col not in ('USD_CLOSE', 'ds', 'y')]\n",
    "for feature in feature_cols:\n",
    "    np_model_last_sample_only.add_lagged_regressor(f'{feature}', only_last_value=True)\n",
    "    \n",
    "np_model_last_sample_only.fit(np_train_df, freq='D')\n",
    "forecasts = collect_np_forecasts(np_model_last_sample_only, test_dataset)\n",
    "np_last_sample_only_error_metrics, fcs = compute_np_error_metrics(forecasts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-theme",
   "metadata": {},
   "source": [
    "Once again, we are able to plot the learned parameters of the model. The lagged regressors are now grouped together in a single chart, as only one value of each is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-electron",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_model_last_sample_only.plot_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automated-blanket",
   "metadata": {},
   "source": [
    "### Plot forecasts at max lead time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forty-cowboy",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_fcs = [{'date': fc.index[-1:][0], 'yhat':fc[-1:][0]} for fc in forecasts]\n",
    "max_fcs = pd.DataFrame(max_fcs)\n",
    "\n",
    "plt.figure(figsize=(12,3))\n",
    "plt.plot(test_df.date, test_df['USD_CLOSE'], color='blue', label='ground truth')\n",
    "plt.plot(max_fcs.date, max_fcs.yhat, color='red', label='forecast')\n",
    "plt.title(f\"Forecasts at max lead time ({lead_time} samples) - Neural Prophet 1-Step Lag\")\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visible-sheet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ground truth\n",
    "plt.figure(figsize=(12,3))\n",
    "ground_truth = test_df[['date', 'USD_CLOSE']]\n",
    "plt.plot(ground_truth.date, ground_truth['USD_CLOSE'], label='ground truth')\n",
    "\n",
    "# Plot example single forecast\n",
    "plt.plot(forecasts[-1], label='forecast')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-discussion",
   "metadata": {},
   "source": [
    "### Append evaluation metrics to `results_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "industrial-context",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results_df.append(compute_error_statistics(np_last_sample_only_error_metrics, 'neural_prophet_last_sample_only')['mean'])\n",
    "results_df.sort_values('mae')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "steady-macedonia",
   "metadata": {
    "id": "rNDiJ6ZZwx6k"
   },
   "source": [
    "## Model with Sparse Neural Autoregression\n",
    "\n",
    "In the previous parameter plots, you could see high values for all autoregressive features. You can tell NeuralProphet to try avoiding relying on them too much by restricting how many of them it is able to use. In this case, we set `ar_reg` to 10, which imposes a strong regularization of the AR model towards sparsity in its coefficients. We also reduce the AR depth to 10 days. *Note*: NeuralProphet applies this sparsity factor only to the regular AR coefficients, not the lagged regressor AR coefficients, where higher sparsity would make more sense.\n",
    "\n",
    "We can also play around with parameters like the number of hidden layers or the learning rate of the AR-Net. Another change applied to this model is the loss function, now MAE instead of the default Huber loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-locking",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_model_sparse_nar = NeuralProphet(n_lags=10, \n",
    "                                    n_forecasts=lead_time,\n",
    "                                    ar_reg=10,\n",
    "                                    learning_rate=5e-3,\n",
    "                                    num_hidden_layers=2,\n",
    "                                    d_hidden=16,\n",
    "                                    loss_func='MAE'\n",
    "                                    )\n",
    "\n",
    "# Add the non-target feature columns as lagged regressors\n",
    "feature_cols = [col for col in np_train_df if col not in ('USD_CLOSE', 'ds', 'y')]\n",
    "for feature in feature_cols:\n",
    "    np_model_sparse_nar.add_lagged_regressor(f'{feature}', n_lags=1)\n",
    "    \n",
    "np_model_sparse_nar.fit(np_train_df, freq='D')\n",
    "forecasts = collect_np_forecasts(np_model_sparse_nar, test_dataset)\n",
    "np_sparse_ar_error_metrics, fcs = compute_np_error_metrics(forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-individual",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_model_sparse_nar.plot_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-memorial",
   "metadata": {},
   "source": [
    "### Plot forecasts at max lead time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-corrections",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "gJszSXMA0NJO",
    "outputId": "55eff862-a844-4622-d509-d0978776eecf"
   },
   "outputs": [],
   "source": [
    "max_fcs = [{'date': fc.index[-1:][0], 'yhat':fc[-1:][0]} for fc in forecasts]\n",
    "max_fcs = pd.DataFrame(max_fcs)\n",
    "\n",
    "plt.figure(figsize=(12,3))\n",
    "plt.plot(test_df.date, test_df['USD_CLOSE'], color='blue', label='ground truth')\n",
    "plt.plot(max_fcs.date, max_fcs.yhat, color='red', label='forecast')\n",
    "plt.title(f\"Forecasts at max lead time ({lead_time} samples) - Neural Prophet Sparse\")\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-mortgage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ground truth\n",
    "plt.figure(figsize=(12,3))\n",
    "ground_truth = test_df[['date', 'USD_CLOSE']]\n",
    "plt.plot(ground_truth.date, ground_truth['USD_CLOSE'], label='ground truth')\n",
    "\n",
    "# Plot example single forecast\n",
    "plt.plot(forecasts[-1], label='forecast')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-hawaiian",
   "metadata": {},
   "source": [
    "### Append evaluation metrics to `results_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-serbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results_df.append(compute_error_statistics(np_sparse_ar_error_metrics, 'neural_prophet_sparse_ar')['mean'])\n",
    "results_df.sort_values('mae')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joint-texture",
   "metadata": {
    "id": "J7GuAAzvwUNF"
   },
   "source": [
    "# Reflections and Next Steps\n",
    "\n",
    "So far, the best performing 'model' is the persistence forecasting model. This is, of course, an unsatisfactory result. The best performing experimental model on the exchange rates dataset is the restricted NeuralProphet model that uses only the last observation of lagged regressors as features. Of course, we have only considered a very small number of configurations using NeuralProphet, many more model and hyperparameter configurations are possible. Please refer to the [NeuralProphet documentation](https://neuralprophet.com/html/contents.html) for detailed information. However, to *find* a better configuration may require significant effort, either manual or automated (via a hyperparameter search, for example). In practical forecasting use cases, it may be important to consider the time, resources, and effort that are needed to find a forecasting model that is better than  baseline.\n",
    "\n",
    "The following notebooks in this series will cover additional models (N-BEATS and DeepAR) as well as rolling cross validation using NeuralProphet. In order to compare the out-of-sample forecasts produced by this notebook to others, the `results_df` DataFrame is saved below. Hopefully we will find a model that performs better than baseline in a continued out-of-sample evaluation experiment!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b269b3",
   "metadata": {},
   "source": [
    "*Note* The path here should be changed to your local drive folder in order for it to be retrieved in other notebooks. See the beginning of this notebook for context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stunning-assistant",
   "metadata": {
    "id": "AWA0tjyfGQWD"
   },
   "outputs": [],
   "source": [
    "output_filename = \"/h/kkoch/forecasting-bootcamp/demos/exchange_rate_mean_test_metrics.csv\"\n",
    "results_df.to_csv(output_filename)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "exchange_dataset.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "e7915587377f13d9edba30ea11b3c98d6b26a63b60775c72ad031792b92d4187"
  },
  "kernelspec": {
   "display_name": "Forecasting Bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
